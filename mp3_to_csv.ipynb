{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9859bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyannote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyannote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyannote'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from pyannote.audio import Audio\n",
    "import csv\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ef98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", \n",
    "                                               use_auth_token=True)\n",
    "\n",
    "model = whisper.load_model(\"medium\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f26bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_csv(mp3_file_path, speaker_diarization, model):\n",
    "    # .mp3 to .wav\n",
    "    file_name = os.path.basename(mp3_file_path)\n",
    "    file_name_without_extension = os.path.splitext(mp3_file_path)[0]\n",
    "    wav_file_path = file_name_without_extension + '.wav'\n",
    "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
    "    audio.export(wav_file_path, format='wav')\n",
    "#     first_minute = Segment(0, 20) # Eases testing if you need to debug/want to poke around with efficacy\n",
    "\n",
    "    # Diarize\n",
    "    who_speaks_when = speaker_diarization(file = wav_file_path, \n",
    "                                      num_speakers=None,  # these values can be\n",
    "                                      min_speakers=None,  # provided by the user\n",
    "                                      max_speakers=None)  # when they are known\n",
    "    \n",
    "    df = pd.DataFrame(columns=['speaker','text'])\n",
    "    audio = Audio(sample_rate=16000, mono=True)\n",
    "    # Transcribe via diarized segments\n",
    "    for segment, _, speaker in who_speaks_when.itertracks(yield_label=True): #.crop(first_minute) for testing\n",
    "        waveform, sample_rate = audio.crop(wav_file_path, segment)\n",
    "        text = model.transcribe(waveform.squeeze().numpy())[\"text\"]\n",
    "\n",
    "        # Append the speaker and text to the DataFrame\n",
    "        df.loc[len(df)] = [speaker, text]\n",
    "    \n",
    "    # Transcription to CSV\n",
    "    df.to_csv(file_name_without_extension+'_transcribed.csv', index=False)\n",
    "    print(f\"successfully transcribed: {mp3_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c842fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample.mp3\n",
      "successfully transcribed: sample.mp3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following will look for all mp3 files in present working directory, \n",
    "and attempt to transcribe>diarize>sheetsify each file. \n",
    "\n",
    "You may have to remove completed files in the event that a subsequent file fails, \n",
    "in order for this to not attempt that file again.\n",
    "'''\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Find all MP3 files in the current directory\n",
    "mp3_files = [file for file in os.listdir(cwd) if file.endswith('.mp3')]\n",
    "\n",
    "for mp3_file_path in mp3_files:\n",
    "    # Create the corresponding CSV file name\n",
    "    csv_file_path = mp3_file_path.replace('.mp3', '_transcribed.csv')\n",
    "    \n",
    "    # Check if the CSV file exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(mp3_file_path)\n",
    "        convert_mp3_to_csv(mp3_file_path, \n",
    "                           speaker_diarization, \n",
    "                           model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
